{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "i = 0\n",
    "filepaths = []\n",
    "# \"C:/Users/janja/Desktop/firstData\"\n",
    "for root, dirs, files in os.walk(\"C:/Users/janja/OneDrive/Pulpit/DaneMGR\", topdown=True):\n",
    "    for name in dirs:\n",
    "        if (bool(re.findall('\\d$', name)) == False):\n",
    "            Path = (root + '/' + name)\n",
    "            filepaths.append(re.sub('DaneMGR\\\\\\\\', 'DaneMGR/', Path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "filenameList = []\n",
    "fullPath = []\n",
    "for Path in filepaths:\n",
    "    for (dirpath, dirnames, filenames) in walk(Path):\n",
    "        for name in filenames:\n",
    "            if (bool(re.findall('fast_Unknown', name)) == True) and name not in filenameList:\n",
    "                NewName = re.sub('._CsvLog', 'CsvLog', name)\n",
    "                filenameList.append(NewName)\n",
    "                fullPath.append(Path + '/' + NewName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/1/B/CsvLogBase_2022-01-15_191202.371_fast_Unknown.csv\n",
      "Imported file number: 1, from files total: 98, and that is 1.02%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/1/R/CsvLogBase_2022-01-15_235231.870_fast_Unknown.csv\n",
      "Imported file number: 2, from files total: 98, and that is 2.04%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/10/B/CsvLogBase_2022-03-22_190208.416_fast_Unknown.csv\n",
      "Imported file number: 3, from files total: 98, and that is 3.06%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/10/R/CsvLogBase_2022-03-22_145436.972_fast_Unknown.csv\n",
      "Imported file number: 4, from files total: 98, and that is 4.08%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/11/B/CsvLogBase_2022-03-24_155741.211_fast_Unknown.csv\n",
      "Imported file number: 5, from files total: 98, and that is 5.10%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/11/R/CsvLogBase_2022-03-23_192319.765_fast_Unknown.csv\n",
      "Imported file number: 6, from files total: 98, and that is 6.12%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/12/B/CsvLogBase_2022-04-03_181938.287_fast_Unknown.csv\n",
      "Imported file number: 7, from files total: 98, and that is 7.14%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/12/R/CsvLogBase_2022-04-03_215403.654_fast_Unknown.csv\n",
      "Imported file number: 8, from files total: 98, and that is 8.16%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/13/B/CsvLogBase_2022-04-04_185513.520_fast_Unknown.csv\n",
      "Imported file number: 9, from files total: 98, and that is 9.18%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/13/R/CsvLogBase_2022-04-04_230427.597_fast_Unknown.csv\n",
      "Imported file number: 10, from files total: 98, and that is 10.20%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/14/B/CsvLogBase_2022-04-06_132729.025_fast_Unknown.csv\n",
      "Imported file number: 11, from files total: 98, and that is 11.22%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/14/R/CsvLogBase_2022-04-05_161416.788_fast_Unknown.csv\n",
      "Imported file number: 12, from files total: 98, and that is 12.24%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/15/B/CsvLogBase_2022-04-07_191904.132_fast_Unknown.csv\n",
      "Imported file number: 13, from files total: 98, and that is 13.27%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/15/R/CsvLogBase_2022-04-07_224810.884_fast_Unknown.csv\n",
      "Imported file number: 14, from files total: 98, and that is 14.29%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/16/B/CsvLogBase_2022-04-08_225003.139_fast_Unknown.csv\n",
      "Imported file number: 15, from files total: 98, and that is 15.31%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/16/R/CsvLogBase_2022-04-08_121001.867_fast_Unknown.csv\n",
      "Imported file number: 16, from files total: 98, and that is 16.33%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/17/B/CsvLogBase_2022-04-25_105956.857_fast_Unknown.csv\n",
      "Imported file number: 17, from files total: 98, and that is 17.35%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/17/R/CsvLogBase_2022-04-25_152538.609_fast_Unknown.csv\n",
      "Imported file number: 18, from files total: 98, and that is 18.37%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/18/B/CsvLogBase_2022-05-05_171713.844_fast_Unknown.csv\n",
      "Imported file number: 19, from files total: 98, and that is 19.39%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/18/R/CsvLogBase_2022-05-05_214033.287_fast_Unknown.csv\n",
      "Imported file number: 20, from files total: 98, and that is 20.41%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/19/B/CsvLogBase_2022-05-09_182534.713_fast_Unknown.csv\n",
      "Imported file number: 21, from files total: 98, and that is 21.43%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/19/R/CsvLogBase_2022-05-09_225045.371_fast_Unknown.csv\n",
      "Imported file number: 22, from files total: 98, and that is 22.45%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/2/B/CsvLogBase_2022-01-21_193358.359_fast_Unknown.csv\n",
      "Imported file number: 23, from files total: 98, and that is 23.47%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/2/R/CsvLogBase_2022-01-18_162025.511_fast_Unknown.csv\n",
      "Imported file number: 24, from files total: 98, and that is 24.49%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/20/B/CsvLogBase_2022-05-30_154146.336_fast_Unknown.csv\n",
      "Imported file number: 25, from files total: 98, and that is 25.51%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/20/R/CsvLogBase_2022-05-30_111345.252_fast_Unknown.csv\n",
      "Imported file number: 26, from files total: 98, and that is 26.53%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/21/B/CsvLogBase_2022-06-08_124008.050_fast_Unknown.csv\n",
      "Imported file number: 27, from files total: 98, and that is 27.55%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/21/R/CsvLogBase_2022-06-07_163230.569_fast_Unknown.csv\n",
      "Imported file number: 28, from files total: 98, and that is 28.57%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/22/B/CsvLogBase_2022-11-10_172101.648_fast_Unknown.csv\n",
      "Imported file number: 29, from files total: 98, and that is 29.59%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/22/R/CsvLogBase_2022-11-10_105441.870_fast_Unknown.csv\n",
      "Imported file number: 30, from files total: 98, and that is 30.61%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/23/B/CsvLogBase_2022-11-21_220743.358_fast_Unknown.csv\n",
      "Imported file number: 31, from files total: 98, and that is 31.63%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/23/R/CsvLogBase_2022-11-21_190104.854_fast_Unknown.csv\n",
      "Imported file number: 32, from files total: 98, and that is 32.65%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/24/B/CsvLogBase_2022-11-29_213923.786_fast_Unknown.csv\n",
      "Imported file number: 33, from files total: 98, and that is 33.67%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/24/R/CsvLogBase_2022-11-30_005133.036_fast_Unknown.csv\n",
      "Imported file number: 34, from files total: 98, and that is 34.69%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/25/B/CsvLogBase_2022-06-13_231553.772_fast_Unknown.csv\n",
      "Imported file number: 35, from files total: 98, and that is 35.71%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/25/R/CsvLogBase_2022-06-13_194556.562_fast_Unknown.csv\n",
      "Imported file number: 36, from files total: 98, and that is 36.73%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/26/B/CsvLogBase_2022-06-14_130618.012_fast_Unknown.csv\n",
      "Imported file number: 37, from files total: 98, and that is 37.76%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/26/R/CsvLogBase_2022-06-14_165454.079_fast_Unknown.csv\n",
      "Imported file number: 38, from files total: 98, and that is 38.78%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/27/B/CsvLogBase_2022-06-17_155205.124_fast_Unknown.csv\n",
      "Imported file number: 39, from files total: 98, and that is 39.80%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/27/R/CsvLogBase_2022-06-17_204342.639_fast_Unknown.csv\n",
      "Imported file number: 40, from files total: 98, and that is 40.82%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/28/B/CsvLogBase_2022-11-17_170602.578_fast_Unknown.csv\n",
      "Imported file number: 41, from files total: 98, and that is 41.84%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/28/R/CsvLogBase_2022-11-17_132241.399_fast_Unknown.csv\n",
      "Imported file number: 42, from files total: 98, and that is 42.86%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/29/B/CsvLogBase_2022-06-26_005021.547_fast_Unknown.csv\n",
      "Imported file number: 43, from files total: 98, and that is 43.88%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/29/R/CsvLogBase_2022-06-25_210412.010_fast_Unknown.csv\n",
      "Imported file number: 44, from files total: 98, and that is 44.90%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/3/B/CsvLogBase_2022-01-27_123020.726_fast_Unknown.csv\n",
      "Imported file number: 45, from files total: 98, and that is 45.92%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/3/R/CsvLogBase_2022-01-26_163953.726_fast_Unknown.csv\n",
      "Imported file number: 46, from files total: 98, and that is 46.94%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/30/B/CsvLogBase_2022-06-27_193800.209_fast_Unknown.csv\n",
      "Imported file number: 47, from files total: 98, and that is 47.96%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/30/R/CsvLogBase_2022-06-27_161352.672_fast_Unknown.csv\n",
      "Imported file number: 48, from files total: 98, and that is 48.98%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/31/B/CsvLogBase_2022-11-18_172548.093_fast_Unknown.csv\n",
      "Imported file number: 49, from files total: 98, and that is 50.00%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/31/R/CsvLogBase_2022-11-19_130602.689_fast_Unknown.csv\n",
      "Imported file number: 50, from files total: 98, and that is 51.02%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/32/B/CsvLogBase_2022-07-07_205459.338_fast_Unknown.csv\n",
      "Imported file number: 51, from files total: 98, and that is 52.04%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/32/R/CsvLogBase_2022-07-07_165933.556_fast_Unknown.csv\n",
      "Imported file number: 52, from files total: 98, and that is 53.06%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/33/B/CsvLogBase_2022-07-09_081037.992_fast_Unknown.csv\n",
      "Imported file number: 53, from files total: 98, and that is 54.08%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/33/R/CsvLogBase_2022-07-09_000038.878_fast_Unknown.csv\n",
      "Imported file number: 54, from files total: 98, and that is 55.10%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/34/B/CsvLogBase_2022-07-11_193122.803_fast_Unknown.csv\n",
      "Imported file number: 55, from files total: 98, and that is 56.12%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/34/R/CsvLogBase_2022-07-11_160255.506_fast_Unknown.csv\n",
      "Imported file number: 56, from files total: 98, and that is 57.14%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/35/B/CsvLogBase_2022-07-30_120208.938_fast_Unknown.csv\n",
      "Imported file number: 57, from files total: 98, and that is 58.16%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/35/R/CsvLogBase_2022-07-31_164452.072_fast_Unknown.csv\n",
      "Imported file number: 58, from files total: 98, and that is 59.18%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/36/B/CsvLogBase_2022-07-31_091040.746_fast_Unknown.csv\n",
      "Imported file number: 59, from files total: 98, and that is 60.20%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/36/R/CsvLogBase_2022-07-31_135636.580_fast_Unknown.csv\n",
      "Imported file number: 60, from files total: 98, and that is 61.22%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/37/B/CsvLogBase_2022-08-01_223106.879_fast_Unknown.csv\n",
      "Imported file number: 61, from files total: 98, and that is 62.24%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/37/R/CsvLogBase_2022-08-01_193357.005_fast_Unknown.csv\n",
      "Imported file number: 62, from files total: 98, and that is 63.27%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/38/B/CsvLogBase_2022-08-06_172157.070_fast_Unknown.csv\n",
      "Imported file number: 63, from files total: 98, and that is 64.29%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/38/R/CsvLogBase_2022-08-06_220857.780_fast_Unknown.csv\n",
      "Imported file number: 64, from files total: 98, and that is 65.31%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/39/B/CsvLogBase_2022-12-08_162059.764_fast_Unknown.csv\n",
      "Imported file number: 65, from files total: 98, and that is 66.33%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/39/R/CsvLogBase_2022-12-06_230318.060_fast_Unknown.csv\n",
      "Imported file number: 66, from files total: 98, and that is 67.35%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/4/B/CsvLogBase_2022-11-19_225923.810_fast_Unknown.csv\n",
      "Imported file number: 67, from files total: 98, and that is 68.37%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/4/R/CsvLogBase_2022-11-19_190114.754_fast_Unknown.csv\n",
      "Imported file number: 68, from files total: 98, and that is 69.39%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/40/B/CsvLogBase_2022-11-18_114055.749_fast_Unknown.csv\n",
      "Imported file number: 69, from files total: 98, and that is 70.41%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/40/R/CsvLogBase_2022-11-18_073907.920_fast_Unknown.csv\n",
      "Imported file number: 70, from files total: 98, and that is 71.43%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/41/B/CsvLogBase_2022-10-07_221301.912_fast_Unknown.csv\n",
      "Imported file number: 71, from files total: 98, and that is 72.45%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/41/R/CsvLogBase_2022-10-09_202711.700_fast_Unknown.csv\n",
      "Imported file number: 72, from files total: 98, and that is 73.47%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/42/B/CsvLogBase_2022-10-13_220905.228_fast_Unknown.csv\n",
      "Imported file number: 73, from files total: 98, and that is 74.49%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/42/R/CsvLogBase_2022-10-13_184911.264_fast_Unknown.csv\n",
      "Imported file number: 74, from files total: 98, and that is 75.51%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/43/B/CsvLogBase_2022-10-14_210337.385_fast_Unknown.csv\n",
      "Imported file number: 75, from files total: 98, and that is 76.53%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/43/R/CsvLogBase_2022-10-14_163627.934_fast_Unknown.csv\n",
      "Imported file number: 76, from files total: 98, and that is 77.55%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/44/B/CsvLogBase_2022-10-17_154710.989_fast_Unknown.csv\n",
      "Imported file number: 77, from files total: 98, and that is 78.57%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/44/R/CsvLogBase_2022-10-17_115328.730_fast_Unknown.csv\n",
      "Imported file number: 78, from files total: 98, and that is 79.59%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/45/B/CsvLogBase_2022-11-02_001717.105_fast_Unknown.csv\n",
      "Imported file number: 79, from files total: 98, and that is 80.61%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/45/R/CsvLogBase_2022-11-01_211857.465_fast_Unknown.csv\n",
      "Imported file number: 80, from files total: 98, and that is 81.63%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/46/B/CsvLogBase_2022-11-06_215817.118_fast_Unknown.csv\n",
      "Imported file number: 81, from files total: 98, and that is 82.65%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/46/R/CsvLogBase_2022-11-06_172531.456_fast_Unknown.csv\n",
      "Imported file number: 82, from files total: 98, and that is 83.67%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/48/B/CsvLogBase_2022-11-16_205505.479_fast_Unknown.csv\n",
      "Imported file number: 83, from files total: 98, and that is 84.69%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/48/R/CsvLogBase_2022-11-16_165828.469_fast_Unknown.csv\n",
      "Imported file number: 84, from files total: 98, and that is 85.71%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/49/B/CsvLogBase_2022-12-07_210322.788_fast_Unknown.csv\n",
      "Imported file number: 85, from files total: 98, and that is 86.73%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/49/R/CsvLogBase_2022-12-07_180529.163_fast_Unknown.csv\n",
      "Imported file number: 86, from files total: 98, and that is 87.76%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/5/B/CsvLogBase_2022-02-01_181231.975_fast_Unknown.csv\n",
      "Imported file number: 87, from files total: 98, and that is 88.78%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/5/R/CsvLogBase_2022-02-02_190309.500_fast_Unknown.csv\n",
      "Imported file number: 88, from files total: 98, and that is 89.80%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/50/B/CsvLogBase_2022-12-09_173341.514_fast_Unknown.csv\n",
      "Imported file number: 89, from files total: 98, and that is 90.82%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/50/R/CsvLogBase_2022-12-09_134319.492_fast_Unknown.csv\n",
      "Imported file number: 90, from files total: 98, and that is 91.84%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/6/B/CsvLogBase_2022-02-05_190838.197_fast_Unknown.csv\n",
      "Imported file number: 91, from files total: 98, and that is 92.86%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/6/R/CsvLogBase_2022-02-04_205828.156_fast_Unknown.csv\n",
      "Imported file number: 92, from files total: 98, and that is 93.88%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/7/B/CsvLogBase_2022-03-27_205305.477_fast_Unknown.csv\n",
      "Imported file number: 93, from files total: 98, and that is 94.90%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/7/R/CsvLogBase_2022-03-26_193506.480_fast_Unknown.csv\n",
      "Imported file number: 94, from files total: 98, and that is 95.92%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/8/B/CsvLogBase_2022-03-18_120627.124_fast_Unknown.csv\n",
      "Imported file number: 95, from files total: 98, and that is 96.94%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/8/R/CsvLogBase_2022-03-18_084504.296_fast_Unknown.csv\n",
      "Imported file number: 96, from files total: 98, and that is 97.96%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/9/B/CsvLogBase_2022-03-21_201003.975_fast_Unknown.csv\n",
      "Imported file number: 97, from files total: 98, and that is 98.98%\n",
      "C:/Users/janja/OneDrive/Pulpit/DaneMGR/9/R/CsvLogBase_2022-03-21_155703.675_fast_Unknown.csv\n",
      "Imported file number: 98, from files total: 98, and that is 100.00%\n"
     ]
    }
   ],
   "source": [
    "T = 'N' #500000 #'N' # describes which rows multiplied by n should be taken into the dataset #If chosen parameter is N the rows will not get dropped\n",
    "fields = ['Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]', 'Infinity|SPO2.SPO2_PULSE [COUNTS]']\n",
    "df_0 = pd.DataFrame()\n",
    "df_1 = pd.DataFrame()\n",
    "\n",
    "files_total = len(fullPath)\n",
    "i = 1\n",
    "for path in fullPath:\n",
    "    print(path)\n",
    "    df_local = pd.read_csv(path, sep = ',', encoding = 'UTF-8', usecols=fields)\n",
    "    if T != 'N':\n",
    "        df_local = df_local[df_local.index % T == 0] #Set to 2000 as 1 second is 20 observations\n",
    "    match = re.findall(\"/B/Csv\",path)\n",
    "    if bool(match) == True:\n",
    "        df_1 = df_1.append(df_local)\n",
    "    else:\n",
    "        df_0 = df_0.append(df_local)\n",
    "    #neo = re.findall('\\/([\\d]{1,2})\\/', path)\n",
    "    #df_local.insert(0,'neonate', str(neo))\n",
    "    #print(df_local)\n",
    "    print(f\"Imported file number: {i}, from files total: {files_total}, and that is {i*100/files_total:.2f}%\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]</th>\n",
       "      <th>Infinity|SPO2.SPO2_PULSE [COUNTS]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.844</td>\n",
       "      <td>937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>931.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.927</td>\n",
       "      <td>919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788087</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788088</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788089</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788090</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2788091</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125870260 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]  \\\n",
       "0                                       -1.844   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                       -1.927   \n",
       "...                                        ...   \n",
       "2788087                                    NaN   \n",
       "2788088                                    NaN   \n",
       "2788089                                    NaN   \n",
       "2788090                                    NaN   \n",
       "2788091                                    NaN   \n",
       "\n",
       "         Infinity|SPO2.SPO2_PULSE [COUNTS]  \n",
       "0                                    937.0  \n",
       "1                                      NaN  \n",
       "2                                    931.0  \n",
       "3                                      NaN  \n",
       "4                                    919.0  \n",
       "...                                    ...  \n",
       "2788087                                NaN  \n",
       "2788088                                NaN  \n",
       "2788089                                NaN  \n",
       "2788090                                NaN  \n",
       "2788091                                NaN  \n",
       "\n",
       "[125870260 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]</th>\n",
       "      <th>Infinity|SPO2.SPO2_PULSE [COUNTS]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.155</td>\n",
       "      <td>-45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.069</td>\n",
       "      <td>-73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935557</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935558</th>\n",
       "      <td>0.064</td>\n",
       "      <td>-235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935559</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935560</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935561</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129254204 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]  \\\n",
       "0                                       -0.155   \n",
       "1                                          NaN   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                       -0.069   \n",
       "...                                        ...   \n",
       "1935557                                    NaN   \n",
       "1935558                                  0.064   \n",
       "1935559                                    NaN   \n",
       "1935560                                    NaN   \n",
       "1935561                                    NaN   \n",
       "\n",
       "         Infinity|SPO2.SPO2_PULSE [COUNTS]  \n",
       "0                                    -45.0  \n",
       "1                                      NaN  \n",
       "2                                    -60.0  \n",
       "3                                      NaN  \n",
       "4                                    -73.0  \n",
       "...                                    ...  \n",
       "1935557                                NaN  \n",
       "1935558                             -235.0  \n",
       "1935559                                NaN  \n",
       "1935560                             -235.0  \n",
       "1935561                                NaN  \n",
       "\n",
       "[129254204 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [df_0,df_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]  \\\n",
      "0                                       -1.844   \n",
      "1                                          NaN   \n",
      "2                                          NaN   \n",
      "3                                          NaN   \n",
      "4                                       -1.927   \n",
      "...                                        ...   \n",
      "2788087                                    NaN   \n",
      "2788088                                    NaN   \n",
      "2788089                                    NaN   \n",
      "2788090                                    NaN   \n",
      "2788091                                    NaN   \n",
      "\n",
      "         Infinity|SPO2.SPO2_PULSE [COUNTS]  \n",
      "0                                    937.0  \n",
      "1                                      NaN  \n",
      "2                                    931.0  \n",
      "3                                      NaN  \n",
      "4                                    919.0  \n",
      "...                                    ...  \n",
      "2788087                                NaN  \n",
      "2788088                                NaN  \n",
      "2788089                                NaN  \n",
      "2788090                                NaN  \n",
      "2788091                                NaN  \n",
      "\n",
      "[125870260 rows x 2 columns],          Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]  \\\n",
      "0                                       -0.155   \n",
      "1                                          NaN   \n",
      "2                                          NaN   \n",
      "3                                          NaN   \n",
      "4                                       -0.069   \n",
      "...                                        ...   \n",
      "1935557                                    NaN   \n",
      "1935558                                  0.064   \n",
      "1935559                                    NaN   \n",
      "1935560                                    NaN   \n",
      "1935561                                    NaN   \n",
      "\n",
      "         Infinity|SPO2.SPO2_PULSE [COUNTS]  \n",
      "0                                    -45.0  \n",
      "1                                      NaN  \n",
      "2                                    -60.0  \n",
      "3                                      NaN  \n",
      "4                                    -73.0  \n",
      "...                                    ...  \n",
      "1935557                                NaN  \n",
      "1935558                             -235.0  \n",
      "1935559                                NaN  \n",
      "1935560                             -235.0  \n",
      "1935561                                NaN  \n",
      "\n",
      "[129254204 rows x 2 columns]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(data[i], end = \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    #data[i] = data[i].rename({'Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]': 'RESP.ONLY_ONE_IN_GROUP[OHM]','Infinity|SPO2.SPO2_PULSE [COUNTS]': 'SPO2.SPO2_PULSE[COUNTS]'}, axis=1)\n",
    "    print(data[i].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].rename({'Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]': 'RESP.ONLY_ONE_IN_GROUP[OHM]','Infinity|SPO2.SPO2_PULSE [COUNTS]': 'SPO2.SPO2_PULSE[COUNTS]'}, axis=1)\n",
    "    print(f'Number of blank spaces for the {i} position: \\n {data[i].isna().sum()}')\n",
    "    print(data[i].dtypes.value_counts())\n",
    "    print(data[i].describe(), end = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the chosen columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "for i in range(len(data)):\n",
    "    orig_data_report = sv.analyze(data[i], pairwise_analysis = 'on')\n",
    "    orig_data_report.show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def segmentation(pd_data, window_size):\n",
    "    np_data = pd_data.to_numpy()\n",
    "    nb_timestamps, nb_sensors = np_data.shape\n",
    "    \n",
    "    #window_size = 100 # Size of the data segments, earlier there was the value of 100\n",
    "    timestamp_idx = 0 # Index along the timestamp dimension\n",
    "    segment_idx = 0 # Index for the segment dimension\n",
    "    \n",
    "    nb_segments = int(math.floor(nb_timestamps/window_size))\n",
    "    print(f'Starting segmentation with a window size of {window_size} resulting in {nb_segments} segments.')\n",
    "    data_to_save = np.zeros((nb_segments,window_size,nb_sensors),dtype=np.float32)\n",
    "\n",
    "    while segment_idx < nb_segments:\n",
    "        data_to_save[segment_idx] = np_data[timestamp_idx:timestamp_idx+window_size,:]\n",
    "        timestamp_idx += window_size\n",
    "        segment_idx += 1\n",
    "    return data_to_save\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To poniżej powinno być tablicą, a na razie nie jest - czyta na razie tylko wartość (chociaż nawet to nie), ale nigdzie ich nie zapisuje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = []\n",
    "for i in range(len(data)):\n",
    "    #shape = np.array(data[i]).shape\n",
    "    #print(shape)\n",
    "    segmented = segmentation(data[i], 100)\n",
    "    #print(segmented)\n",
    "    position.append(segmented)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(position)):\n",
    "    print(position[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(position)):\n",
    "    position[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(position)):\n",
    "    chunk_size = position[i].shape[1]\n",
    "    print(chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving array for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save_copy = position.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numba import cuda\n",
    "import scipy\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.integrate import trapz\n",
    "from scipy.fft import fft\n",
    "\n",
    "global num_features\n",
    "num_features = 10\n",
    "def feature_extraction(data):\n",
    "        freq = 100\n",
    "        mean_array = np.empty((len(data),2), dtype = float)\n",
    "        median_array = np.empty((len(data),2), dtype = float)\n",
    "        std_array = np.empty((len(data),2), dtype = float)\n",
    "        min_val_array = np.empty((len(data),2), dtype = float)\n",
    "        max_val_array = np.empty((len(data),2), dtype = float)\n",
    "        sum_val_array = np.empty((len(data),2), dtype = float)\n",
    "        kurtosis_array = np.empty((len(data),2), dtype = float)\n",
    "        skewness_array = np.empty((len(data),2), dtype = float)\n",
    "        for i in range(len(data)):\n",
    "                mean_array[i] = np.mean(data[i], axis=0)\n",
    "                median_array[i] = np.median(data[i], axis=0)\n",
    "                std_array[i] = np.std(data[i], axis=0)\n",
    "                min_val_array[i] = np.min(data[i], axis=0)\n",
    "                max_val_array[i] = np.max(data[i], axis=0)\n",
    "                sum_val_array[i] = np.sum(data[i], axis=0)/freq\n",
    "                \n",
    "                kurtosis_array[i] = scipy.stats.kurtosis(data[i], axis=0)\n",
    "                skewness_array[i] = scipy.stats.skew(data[i], axis=0)\n",
    "\n",
    "\n",
    "        fft_sums = np.zeros((len(data),2), dtype = float)\n",
    "\n",
    "        fft_freqs = np.zeros((len(data),2), dtype = float)\n",
    "\n",
    "        #Flattening the array to shape (a*b, c), where c is the number of sensors\n",
    "        flat_arr = data.reshape(len(data)*len(data[1]), len(data[0][1]))\n",
    "\n",
    "        \n",
    "        k = 0\n",
    "        for k in range(len(data)):\n",
    "                \n",
    "                chunk = flat_arr[k * len(data[1]) : (k + 1) * len(data[1]), ]\n",
    "                \n",
    "                sensor_1 = chunk[:,0]\n",
    "                sensor_2 = chunk[:,1]\n",
    "                \n",
    "                \n",
    "                sp1 = np.fft.fft(sensor_1)\n",
    "                ps1 = np.abs(sp1)**2\n",
    "                sp2 = np.fft.fft(sensor_2)\n",
    "                ps2 = np.abs(sp2)**2\n",
    "                \n",
    "                \n",
    "                # Define the frequency range of interest\n",
    "                sensor_1_freq = np.fft.fftfreq(len(chunk), 1/freq)\n",
    "                \n",
    "                idx1 = np.logical_and(sensor_1_freq >= 0, sensor_1_freq <= freq)\n",
    "\n",
    "                # Integrate the power spectrum over the frequency range of interest\n",
    "                area1 = trapz(ps1[idx1], sensor_1_freq[idx1])\n",
    "\n",
    "\n",
    "                # Define the frequency range of interest\n",
    "                sensor_2_freq = np.fft.fftfreq(len(chunk), 1/freq)\n",
    "                \n",
    "                idx2 = np.logical_and(sensor_2_freq >= 0, sensor_2_freq <= freq)\n",
    "\n",
    "                # Integrate the power spectrum over the frequency range of interest\n",
    "                area2 = trapz(ps2[idx1], sensor_2_freq[idx1])\n",
    "\n",
    "\n",
    "                fft_sums[k] = [area1, area2]\n",
    "                \n",
    "                \n",
    "                argmax_ind_1 = np.arange(len(sp1))\n",
    "                argmax_list_1 = argmax_ind_1[np.argsort(-np.abs(sp1))]\n",
    "                \n",
    "                max_power_frequency_1 = argmax_list_1[0] * (freq / len(chunk))\n",
    "                \n",
    "                \n",
    "                argmax_ind_2 = np.arange(len(sp2))\n",
    "                argmax_list_2 = argmax_ind_2[np.argsort(-np.abs(sp2))]\n",
    "                \n",
    "                \n",
    "                array_len_1 = len(argmax_list_1)\n",
    "                \n",
    "                for ind_1 in range(array_len_1):\n",
    "                        \n",
    "                        if (argmax_list_1[ind_1] * (freq / len(chunk)) <= 1):\n",
    "                                \n",
    "                                continue\n",
    "                        \n",
    "                        elif (argmax_list_1[ind_1] * (freq / len(chunk)) > 1):\n",
    "                                max_power_frequency_1 = argmax_list_1[ind_1] * (freq / len(chunk))\n",
    "\n",
    "                                break\n",
    "                        \n",
    "                array_len_2 = len(argmax_list_2)\n",
    "\n",
    "                for ind_2 in range(array_len_2):\n",
    "                        \n",
    "                        if (argmax_list_2[ind_2] * (freq / len(chunk)) <= 1):\n",
    "                                \n",
    "                                continue\n",
    "                        \n",
    "                        elif (argmax_list_2[ind_2] * (freq / len(chunk)) > 1):\n",
    "                                max_power_frequency_2 = argmax_list_2[ind_2] * (freq / len(chunk))\n",
    "        \n",
    "                                break\n",
    "           \n",
    "        \n",
    "        \n",
    "                fft_freqs[k] = [max_power_frequency_1, max_power_frequency_2]\n",
    "\n",
    "\n",
    "        global feature_names\n",
    "        feature_names = ['mean', 'mean_2', 'median', 'median_2', 'std', 'std_2', 'min_val', 'min_val_2',\\\n",
    "                'max_val', 'max_val_2', 'sum_val', 'sum_val_2', 'kurtosis', 'kurtosis_2', 'skew', 'skew_2', 'fft_sum', 'fft_sum_2', 'argmax_freq', 'argmax_freq_2']\n",
    "        \n",
    "        features = np.zeros((len(data), num_features))        \n",
    "\n",
    "        features = np.concatenate((mean_array, median_array, std_array, min_val_array, max_val_array, sum_val_array, kurtosis_array, skewness_array, fft_sums, fft_freqs), axis = 1)\n",
    "        \n",
    "        return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(position)):\n",
    "        print('Position %d' % i)\n",
    "        feature_extraction(position[i])\n",
    "        x = feature_extraction(position[i])\n",
    "        print(x)\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extract_features(data_to_save, chunk_size):\n",
    "    num_chunks = data_to_save.shape[0]//chunk_size\n",
    "    num_features = 4\n",
    "    num_sensors = data_to_save.shape[2]\n",
    "    features = np.zeros((num_chunks, num_features*num_sensors))\n",
    "    for i in range(num_chunks):\n",
    "        chunk = data_to_save[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "        mean = np.mean(chunk, axis=(0, 1))\n",
    "        median = np.median(chunk, axis=(0, 1))\n",
    "        std = np.std(chunk, axis=(0, 1))\n",
    "        min_val = np.min(chunk, axis=(0, 1))\n",
    "        #arg_max = np.argmax(chunk, axis=(0, 1))\n",
    "        features[i,:] = np.hstack([mean, median, std, min_val]).flatten()\n",
    "    features = features.reshape(num_chunks, num_features, num_sensors)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling():\n",
    "    extracted_features = []\n",
    "    for i in range(len(position)):\n",
    "        extracted_features.append(feature_extraction(position[i]))\n",
    "        #print(extracted_features)\n",
    "    \n",
    "    position_0 = feature_extraction(position[0])\n",
    "    position_1 = feature_extraction(position[1])\n",
    "    \n",
    "    labels_0 = np.zeros(position_0.shape[0])\n",
    "    labels_1 = np.ones(position_1.shape[0])\n",
    "    \n",
    "    complete_feature_dataset = np.concatenate((position_0,position_1))\n",
    "    complete_label_dataset = np.concatenate((labels_0, labels_1))\n",
    "    \n",
    "    return complete_feature_dataset, complete_label_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#labels = np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating an additional table called Saved_data for further actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved_data = complete_feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_to_save = np.array(data_to_save) # this is the 3D array\n",
    "#data_to_save = data_to_save.reshape(data_to_save.shape[0], -1) # reshape to 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def data_shuffling():\n",
    "    dataset = labeling()\n",
    "    shuffler = np.random.permutation(len(dataset[0]))\n",
    "    X = dataset[0][shuffler]\n",
    "    y = dataset[1][shuffler]\n",
    "\n",
    "    return X,y\n",
    "#le = LabelEncoder()\n",
    "#y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_shuffling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_elimination(n = i):\n",
    "    # Create the Random Forest classifier\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    # Perform feature selection using RFE\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n, step=1)\n",
    "    shuffled_dataset = data_shuffling()\n",
    "    X = shuffled_dataset[0]\n",
    "    y = shuffled_dataset[1]\n",
    "    rfe.fit(X, y)\n",
    "\n",
    "    # Get the selected feature indices\n",
    "    selected_features = rfe.support_\n",
    "    selected_features_indices = np.where(selected_features)[0]\n",
    "    print(selected_features_indices)\n",
    "    print('Number of features selected: %d' % (n))\n",
    "    \n",
    "    names_selected_features = []\n",
    "\n",
    "\n",
    "    for i in selected_features_indices:\n",
    "        names_selected_features.append(feature_names[i])\n",
    "    \n",
    "    \n",
    "    print('Features selected: ')\n",
    "    print(names_selected_features)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    # Use the selected features to train and evaluate the classifier\n",
    "    global X_selected\n",
    "    X_selected = X[:, selected_features]\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Train the Random Forest classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy: %.4f%%\" % (accuracy * 100.0))\n",
    "    print(\"Precision: %.4f%%\" % (precision * 100.0))\n",
    "    print(\"Recall: %.4f%%\" % (recall * 100.0))\n",
    "    print(\"Mean Absolute Error:\", mae, end = '\\n')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X_selected, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def xyz():\n",
    "for i in range(num_features*2,0,-1):\n",
    "    feature_elimination(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code opens a mini window in the upper part of the screen\n",
    "n = int(input(\"Choose, with how many features do you want to continue\"))\n",
    "feature_elimination(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.size)\n",
    "print(X_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.size)\n",
    "print(y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "rfc_prediction = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, rfc_prediction)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, rfc_prediction)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, rfc_prediction)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, rfc_prediction)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# create the model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# create the model\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Create an instance of the KFold class\n",
    "kf = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RFC model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_RFC = []\n",
    "precision_scores_RFC = []\n",
    "recall_scores_RFC = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rfc.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_RFC.append(acc)\n",
    "    precision_scores_RFC.append(precision)\n",
    "    recall_scores_RFC.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_RFC))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_RFC))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_RFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGboost model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_XGB = []\n",
    "precision_scores_XGB = []\n",
    "recall_scores_XGB = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_XGB.append(acc)\n",
    "    precision_scores_XGB.append(precision)\n",
    "    recall_scores_XGB.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_XGB))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_XGB))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_XGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVM model\n",
    "SVM = svm.SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_SVC = []\n",
    "precision_scores_SVC = []\n",
    "recall_scores_SVC = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    SVM.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = SVM.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_SVC.append(acc)\n",
    "    precision_scores_SVC.append(precision)\n",
    "    recall_scores_SVC.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_SVC))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_SVC))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the FFT of the signal along the first axis for channel 0\n",
    "fft0 = np.fft.fft(Saved_data[:,:,0], axis=0)\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for channel 1\n",
    "fft1 = np.fft.fft(Saved_data[:,:,1], axis=0)\n",
    "\n",
    "# Compute the power spectrum for channel 0\n",
    "power_spectrum0 = np.abs(fft0)**2\n",
    "\n",
    "# Compute the power spectrum for channel 1\n",
    "power_spectrum1 = np.abs(fft1)**2\n",
    "\n",
    "plots = 25\n",
    "# Plot the power spectrum of 25 slices along the first axis for channel 0\n",
    "for i in range(plots):\n",
    "    plt.plot(power_spectrum0[:, i])\n",
    "    plt.xlabel('Frequency (bins)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 0 (Slice {i})')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the power spectrum of 5 slices along the first axis for channel 1\n",
    "for i in range(plots):\n",
    "    plt.plot(power_spectrum1[:, i])\n",
    "    plt.xlabel('Frequency (bins)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 1 (Slice {i})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the FFT of the signal along the first axis for channel 0\n",
    "fft0 = np.fft.fft(Saved_data[:,:,0], axis=0)\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for channel 1\n",
    "fft1 = np.fft.fft(Saved_data[:,:,1], axis=0)\n",
    "\n",
    "# Compute the power spectrum for channel 0\n",
    "power_spectrum0 = np.abs(fft0)**2\n",
    "\n",
    "# Compute the power spectrum for channel 1\n",
    "power_spectrum1 = np.abs(fft1)**2\n",
    "\n",
    "# Sampling rate\n",
    "sampling_rate = 200\n",
    "\n",
    "# Number of samples\n",
    "N = Saved_data.shape[0]\n",
    "\n",
    "# Compute the frequencies in Hz for the power spectrum\n",
    "frequencies0 = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "frequencies1 = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "\n",
    "plots = 25\n",
    "# Plot the power spectrum of 25 slices along the first axis for channel 0\n",
    "for i in range(plots):\n",
    "    plt.plot(frequencies0, power_spectrum0[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 0 (Slice {i})')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the power spectrum of 5 slices along the first axis for channel 1\n",
    "for i in range(plots):\n",
    "    plt.plot(frequencies1, power_spectrum1[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 1 (Slice {i})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots prepared for newly preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for each sensor\n",
    "fft = np.fft.fft(new_array, axis=0)\n",
    "\n",
    "# Compute the power spectrum for each sensor\n",
    "power_spectrum = np.abs(fft)**2\n",
    "\n",
    "# Sampling rate\n",
    "sampling_rate = 1\n",
    "\n",
    "# Number of samples\n",
    "N = new_array.shape[0]\n",
    "\n",
    "# Compute the frequencies in Hz for the power spectrum\n",
    "frequencies = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "\n",
    "# Plot the power spectrum for each sensor\n",
    "for i in range(new_array.shape[1]):\n",
    "    plt.plot(frequencies, power_spectrum[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Sensor {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
