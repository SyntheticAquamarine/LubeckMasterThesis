{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.integrate import trapz\n",
    "from scipy.fft import fft\n",
    "\n",
    "global num_features\n",
    "num_features = 10\n",
    "def feature_extraction(data):\n",
    "        freq = 100\n",
    "        mean_array = np.zeros((len(data),2), dtype = float)\n",
    "        median_array = np.zeros((len(data),2), dtype = float)\n",
    "        std_array = np.zeros((len(data),2), dtype = float)\n",
    "        min_val_array = np.zeros((len(data),2), dtype = float)\n",
    "        max_val_array = np.zeros((len(data),2), dtype = float)\n",
    "        sum_val_array = np.zeros((len(data),2), dtype = float)\n",
    "        kurtosis_array = np.zeros((len(data),2), dtype = float)\n",
    "        skewness_array = np.zeros((len(data),2), dtype = float)\n",
    "        for i in range(len(data)):\n",
    "                mean_array[i] = np.mean(data[i], axis=0)\n",
    "                median_array[i] = np.median(data[i], axis=0)\n",
    "                std_array[i] = np.std(data[i], axis=0)\n",
    "                min_val_array[i] = np.min(data[i], axis=0)\n",
    "                max_val_array[i] = np.max(data[i], axis=0)\n",
    "                sum_val_array[i] = np.sum(data[i], axis=0)/freq\n",
    "                \n",
    "                kurtosis_array[i] = scipy.stats.kurtosis(data[i], axis=0)       \n",
    "                skewness_array[i] = scipy.stats.skew(data[i], axis=0)\n",
    "\n",
    "\n",
    "        fft_sums = np.zeros((len(data),2), dtype = float)\n",
    "\n",
    "        fft_freqs = np.zeros((len(data),2), dtype = float) ###Czy tu aby na pewno powinien byÄ‡ dloat, a nie liczba urojona\n",
    "\n",
    "        #Flattening the array to shape (a*b, c), where c is the number of sensors\n",
    "        flat_arr = data.reshape(len(data)*len(data[1]), len(data[0][1]))\n",
    "\n",
    "        \n",
    "        k = 0\n",
    "        for k in range(len(data)):\n",
    "                \n",
    "                chunk = flat_arr[k * len(data[1]) : (k + 1) * len(data[1]), ]\n",
    "                \n",
    "                sensor_1 = chunk[:,0]\n",
    "                sensor_2 = chunk[:,1]\n",
    "                \n",
    "                \n",
    "                sp1 = np.fft.fft(sensor_1)\n",
    "                ps1 = np.abs(sp1)**2\n",
    "                sp2 = np.fft.fft(sensor_2)\n",
    "                ps2 = np.abs(sp2)**2\n",
    "                \n",
    "                \n",
    "                # Define the frequency range of interest\n",
    "                sensor_1_freq = np.fft.fftfreq(len(chunk), 1/freq)\n",
    "                \n",
    "                idx1 = np.logical_and(sensor_1_freq >= 0, sensor_1_freq <= freq)\n",
    "\n",
    "                # Integrate the power spectrum over the frequency range of interest\n",
    "                area1 = trapz(ps1[idx1], sensor_1_freq[idx1])\n",
    "\n",
    "\n",
    "                # Define the frequency range of interest\n",
    "                sensor_2_freq = np.fft.fftfreq(len(chunk), 1/freq)\n",
    "                \n",
    "                idx2 = np.logical_and(sensor_2_freq >= 0, sensor_2_freq <= freq)\n",
    "\n",
    "                # Integrate the power spectrum over the frequency range of interest\n",
    "                area2 = trapz(ps2[idx1], sensor_2_freq[idx1])\n",
    "\n",
    "\n",
    "                fft_sums[k] = [area1, area2]\n",
    "                \n",
    "                \n",
    "                argmax_ind_1 = np.arange(len(sp1))\n",
    "                argmax_list_1 = argmax_ind_1[np.argsort(-np.abs(sp1))]\n",
    "                \n",
    "                max_power_frequency_1 = argmax_list_1[0] * (freq / len(chunk))\n",
    "                \n",
    "                \n",
    "                argmax_ind_2 = np.arange(len(sp2))\n",
    "                argmax_list_2 = argmax_ind_2[np.argsort(-np.abs(sp2))]\n",
    "                \n",
    "                \n",
    "                array_len_1 = len(argmax_list_1)\n",
    "                \n",
    "                for ind_1 in range(array_len_1):\n",
    "                        \n",
    "                        if (argmax_list_1[ind_1] * (freq / len(chunk)) <= 1):\n",
    "                                \n",
    "                                continue\n",
    "                        \n",
    "                        elif (argmax_list_1[ind_1] * (freq / len(chunk)) > 1):\n",
    "                                max_power_frequency_1 = argmax_list_1[ind_1] * (freq / len(chunk))\n",
    "\n",
    "                                break\n",
    "                        \n",
    "                array_len_2 = len(argmax_list_2)\n",
    "\n",
    "                for ind_2 in range(array_len_2):\n",
    "                        \n",
    "                        if (argmax_list_2[ind_2] * (freq / len(chunk)) <= 1):\n",
    "                                \n",
    "                                continue\n",
    "                        \n",
    "                        elif (argmax_list_2[ind_2] * (freq / len(chunk)) > 1):\n",
    "                                max_power_frequency_2 = argmax_list_2[ind_2] * (freq / len(chunk))\n",
    "        \n",
    "                                break\n",
    "           \n",
    "        \n",
    "        \n",
    "                fft_freqs[k] = [max_power_frequency_1, max_power_frequency_2]\n",
    "\n",
    "\n",
    "        global feature_names\n",
    "        feature_names = ['mean', 'mean_2', 'median', 'median_2', 'std', 'std_2', 'min_val', 'min_val_2',\\\n",
    "                'max_val', 'max_val_2', 'sum_val', 'sum_val_2', 'kurtosis', 'kurtosis_2', 'skew', 'skew_2', 'fft_sum', 'fft_sum_2', 'argmax_freq', 'argmax_freq_2']\n",
    "        \n",
    "        features = np.zeros((len(data), num_features))        \n",
    "\n",
    "        features = np.concatenate((mean_array, median_array, std_array, min_val_array, max_val_array, sum_val_array, kurtosis_array, skewness_array, fft_sums, fft_freqs), axis = 1)\n",
    "        \n",
    "        return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = './CreatedFiles/Segmentation/'\n",
    "\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('_B.npy'):\n",
    "        file_loaded = np.load(data_dir + filename)\n",
    "        extracted = feature_extraction(file_loaded)\n",
    "        \n",
    "        location_FE_B = f'./CreatedFiles/Extracted_Features/{filename}'\n",
    "        np.save(location_FE_B, extracted)\n",
    "    elif filename.endswith('_R.npy'):\n",
    "        file_loaded = np.load(data_dir + filename)\n",
    "        extracted = feature_extraction(file_loaded)\n",
    "        \n",
    "        location_FE_R = f'./CreatedFiles/Extracted_Features/{filename}'\n",
    "        np.save(location_FE_R, extracted)\n",
    "        continue\n",
    "    else:\n",
    "        print('Error file not found')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling():\n",
    "    extracted_features = []\n",
    "    for i in range(len(position)):\n",
    "        extracted_features.append(feature_extraction(position[i]))\n",
    "        #print(extracted_features)\n",
    "    \n",
    "    position_0 = feature_extraction(position[0])\n",
    "    position_1 = feature_extraction(position[1])\n",
    "    \n",
    "    labels_0 = np.zeros(position_0.shape[0])\n",
    "    labels_1 = np.ones(position_1.shape[0])\n",
    "    \n",
    "    complete_feature_dataset = np.concatenate((position_0,position_1))\n",
    "    complete_label_dataset = np.concatenate((labels_0, labels_1))\n",
    "    \n",
    "    return complete_feature_dataset, complete_label_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28388/1287467609.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_B.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mfile_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_loaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "data_dir = './CreatedFiles/Extracted_Features/'\n",
    "\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('_B.npy'):\n",
    "        \n",
    "        file_loaded = np.load(data_dir + filename)\n",
    "        labels = np.zeros(file_loaded.shape[0])\n",
    "        \n",
    "        label_file = re.sub('(\\.npy)','',filename)\n",
    "        #location_FE_B = f'./CreatedFiles/Labels/{filename}'\n",
    "        #np.save(location_FE_B, extracted)\n",
    "        \n",
    "    elif filename.endswith('_R.npy'):\n",
    "        \n",
    "        #file_loaded = np.load(data_dir + filename)\n",
    "        #labels = np.zeros(file_loaded.shape[0])\n",
    "    \n",
    "        #location_FE_R = f'./CreatedFiles/Labels/{filename}'\n",
    "        #np.save(location_FE_R, extracted)\n",
    "        continue\n",
    "    else:\n",
    "        print('Filename error')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#labels = np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating an additional table called Saved_data for further actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved_data = complete_feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_to_save = np.array(data_to_save) # this is the 3D array\n",
    "#data_to_save = data_to_save.reshape(data_to_save.shape[0], -1) # reshape to 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def data_shuffling():\n",
    "    dataset = labeling()\n",
    "    shuffler = np.random.permutation(len(dataset[0]))\n",
    "    X = dataset[0][shuffler]\n",
    "    y = dataset[1][shuffler]\n",
    "\n",
    "    return X,y\n",
    "#le = LabelEncoder()\n",
    "#y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_shuffling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_elimination(n = i):\n",
    "    # Create the Random Forest classifier\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    # Perform feature selection using RFE\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n, step=1)\n",
    "    shuffled_dataset = data_shuffling()\n",
    "    X = shuffled_dataset[0]\n",
    "    y = shuffled_dataset[1]\n",
    "    rfe.fit(X, y)\n",
    "\n",
    "    # Get the selected feature indices\n",
    "    selected_features = rfe.support_\n",
    "    selected_features_indices = np.where(selected_features)[0]\n",
    "    print(selected_features_indices)\n",
    "    print('Number of features selected: %d' % (n))\n",
    "    \n",
    "    names_selected_features = []\n",
    "\n",
    "\n",
    "    for i in selected_features_indices:\n",
    "        names_selected_features.append(feature_names[i])\n",
    "    \n",
    "    \n",
    "    print('Features selected: ')\n",
    "    print(names_selected_features)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    # Use the selected features to train and evaluate the classifier\n",
    "    global X_selected\n",
    "    X_selected = X[:, selected_features]\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Train the Random Forest classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy: %.4f%%\" % (accuracy * 100.0))\n",
    "    print(\"Precision: %.4f%%\" % (precision * 100.0))\n",
    "    print(\"Recall: %.4f%%\" % (recall * 100.0))\n",
    "    print(\"Mean Absolute Error:\", mae, end = '\\n')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X_selected, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def xyz():\n",
    "for i in range(num_features*2,0,-1):\n",
    "    feature_elimination(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code opens a mini window in the upper part of the screen\n",
    "n = int(input(\"Choose, with how many features do you want to continue\"))\n",
    "feature_elimination(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.size)\n",
    "print(X_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.size)\n",
    "print(y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "rfc_prediction = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, rfc_prediction)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, rfc_prediction)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, rfc_prediction)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, rfc_prediction)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# create the model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# create the model\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Create an instance of the KFold class\n",
    "kf = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RFC model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_RFC = []\n",
    "precision_scores_RFC = []\n",
    "recall_scores_RFC = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rfc.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_RFC.append(acc)\n",
    "    precision_scores_RFC.append(precision)\n",
    "    recall_scores_RFC.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_RFC))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_RFC))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_RFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGboost model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_XGB = []\n",
    "precision_scores_XGB = []\n",
    "recall_scores_XGB = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_XGB.append(acc)\n",
    "    precision_scores_XGB.append(precision)\n",
    "    recall_scores_XGB.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_XGB))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_XGB))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_XGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVM model\n",
    "SVM = svm.SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_SVC = []\n",
    "precision_scores_SVC = []\n",
    "recall_scores_SVC = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    SVM.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = SVM.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_SVC.append(acc)\n",
    "    precision_scores_SVC.append(precision)\n",
    "    recall_scores_SVC.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_SVC))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_SVC))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the FFT of the signal along the first axis for channel 0\n",
    "fft0 = np.fft.fft(Saved_data[:,:,0], axis=0)\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for channel 1\n",
    "fft1 = np.fft.fft(Saved_data[:,:,1], axis=0)\n",
    "\n",
    "# Compute the power spectrum for channel 0\n",
    "power_spectrum0 = np.abs(fft0)**2\n",
    "\n",
    "# Compute the power spectrum for channel 1\n",
    "power_spectrum1 = np.abs(fft1)**2\n",
    "\n",
    "plots = 25\n",
    "# Plot the power spectrum of 25 slices along the first axis for channel 0\n",
    "for i in range(plots):\n",
    "    plt.plot(power_spectrum0[:, i])\n",
    "    plt.xlabel('Frequency (bins)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 0 (Slice {i})')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the power spectrum of 5 slices along the first axis for channel 1\n",
    "for i in range(plots):\n",
    "    plt.plot(power_spectrum1[:, i])\n",
    "    plt.xlabel('Frequency (bins)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 1 (Slice {i})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the FFT of the signal along the first axis for channel 0\n",
    "fft0 = np.fft.fft(Saved_data[:,:,0], axis=0)\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for channel 1\n",
    "fft1 = np.fft.fft(Saved_data[:,:,1], axis=0)\n",
    "\n",
    "# Compute the power spectrum for channel 0\n",
    "power_spectrum0 = np.abs(fft0)**2\n",
    "\n",
    "# Compute the power spectrum for channel 1\n",
    "power_spectrum1 = np.abs(fft1)**2\n",
    "\n",
    "# Sampling rate\n",
    "sampling_rate = 200\n",
    "\n",
    "# Number of samples\n",
    "N = Saved_data.shape[0]\n",
    "\n",
    "# Compute the frequencies in Hz for the power spectrum\n",
    "frequencies0 = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "frequencies1 = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "\n",
    "plots = 25\n",
    "# Plot the power spectrum of 25 slices along the first axis for channel 0\n",
    "for i in range(plots):\n",
    "    plt.plot(frequencies0, power_spectrum0[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 0 (Slice {i})')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the power spectrum of 5 slices along the first axis for channel 1\n",
    "for i in range(plots):\n",
    "    plt.plot(frequencies1, power_spectrum1[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 1 (Slice {i})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots prepared for newly preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for each sensor\n",
    "fft = np.fft.fft(new_array, axis=0)\n",
    "\n",
    "# Compute the power spectrum for each sensor\n",
    "power_spectrum = np.abs(fft)**2\n",
    "\n",
    "# Sampling rate\n",
    "sampling_rate = 1\n",
    "\n",
    "# Number of samples\n",
    "N = new_array.shape[0]\n",
    "\n",
    "# Compute the frequencies in Hz for the power spectrum\n",
    "frequencies = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "\n",
    "# Plot the power spectrum for each sensor\n",
    "for i in range(new_array.shape[1]):\n",
    "    plt.plot(frequencies, power_spectrum[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Sensor {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
