{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "i = 0\n",
    "filepaths = []\n",
    "# \"C:/Users/janja/Desktop/firstData\"\n",
    "for root, dirs, files in os.walk(\"C:/Users/janja/OneDrive/Pulpit/DaneMGR\", topdown=True):\n",
    "    for name in dirs:\n",
    "        if (bool(re.findall('\\d$', name)) == False):\n",
    "            Path = (root + '/' + name)\n",
    "            filepaths.append(re.sub('DaneMGR\\\\\\\\', 'DaneMGR/', Path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "filenameList = []\n",
    "fullPath = []\n",
    "for Path in filepaths:\n",
    "    for (dirpath, dirnames, filenames) in walk(Path):\n",
    "        for name in filenames:\n",
    "            if (bool(re.findall('fast_Unknown', name)) == True) and name not in filenameList:\n",
    "                NewName = re.sub('._CsvLog', 'CsvLog', name)\n",
    "                filenameList.append(NewName)\n",
    "                fullPath.append(Path + '/' + NewName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(filepaths)\n",
    "print('\\n')\n",
    "print(filenameList)\n",
    "print('\\n')\n",
    "print(fullPath)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df = pd.DataFrame()\n",
    "i = 0\n",
    "for path in fullPath:\n",
    "    df_local = pd.read_csv(path, sep = ',', encoding = 'UTF-8')\n",
    "    print(df_local)\n",
    "    match = re.findall(\"/B/Csv\",path)\n",
    "    boolean = bool(match)\n",
    "    print(boolean)\n",
    "    if boolean == True:\n",
    "        df_local['position'] = 1\n",
    "    else:\n",
    "        df_local['position'] = 0\n",
    "    # print(df_local)\n",
    "    i += 1\n",
    "    print(i)\n",
    "    df = df.append(df_local)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T = 5000 # describes which rows multiplied by n should be taken into the dataset\n",
    "fields = ['Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]', 'Infinity|SPO2.SPO2_PULSE [COUNTS]']\n",
    "df = pd.DataFrame()\n",
    "files_total = len(fullPath)\n",
    "i = 1\n",
    "for path in fullPath:\n",
    "    print(path)\n",
    "    df_local = pd.read_csv(path, sep = ',', encoding = 'UTF-8', usecols=fields)\n",
    "    df_local = df_local[df_local.index % T == 0] #Set to 2000 as 1 second is 20 observations\n",
    "    match = re.findall(\"/B/Csv\",path)\n",
    "    if bool(match) == True:\n",
    "        df_local['position'] = 1\n",
    "    else:\n",
    "        df_local['position'] = 0\n",
    "    neo = re.findall('\\/([\\d]{1,2})\\/', path)\n",
    "    df_local.insert(0,'neonate', str(neo))\n",
    "    #print(df_local)\n",
    "    print(f\"Imported file number: {i}, from files total: {files_total}, and that is {i*100/files_total:.2f}%\")\n",
    "    df = df.append(df_local)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 'N' # describes which rows multiplied by n should be taken into the dataset #If chosen parameter is N the rows will not get dropped\n",
    "fields = ['Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]', 'Infinity|SPO2.SPO2_PULSE [COUNTS]']\n",
    "df_0 = pd.DataFrame()\n",
    "df_1 = pd.DataFrame()\n",
    "\n",
    "files_total = len(fullPath)\n",
    "i = 1\n",
    "for path in fullPath:\n",
    "    print(path)\n",
    "    df_local = pd.read_csv(path, sep = ',', encoding = 'UTF-8', usecols=fields)\n",
    "    if T != 'N':\n",
    "        df_local = df_local[df_local.index % T == 0] #Set to 2000 as 1 second is 20 observations\n",
    "    match = re.findall(\"/B/Csv\",path)\n",
    "    if bool(match) == True:\n",
    "        df_1 = df_1.append(df_local)\n",
    "    else:\n",
    "        df_0 = df_0.append(df_local)\n",
    "    #neo = re.findall('\\/([\\d]{1,2})\\/', path)\n",
    "    #df_local.insert(0,'neonate', str(neo))\n",
    "    #print(df_local)\n",
    "    print(f\"Imported file number: {i}, from files total: {files_total}, and that is {i*100/files_total:.2f}%\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [df_0,df_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(data[i], end = \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_Repaired = df.rename({'Infinity|ECG.ECG_LEAD_I [MICROVOLT]': 'ECG.ECG_LEAD_I[MICROVOLT]', 'Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]': 'RESP.ONLY_ONE_IN_GROUP[OHM]','Infinity|SPO2.SPO2_PULSE [COUNTS]': 'SPO2.SPO2_PULSE[COUNTS]'}, axis=1)#.dropna()\n",
    "#FinalData = df_Repaired[['ECG.ECG_LEAD_I[MICROVOLT]', 'RESP.ONLY_ONE_IN_GROUP[OHM]', 'SPO2.SPO2_PULSE[COUNTS]']].copy()\n",
    "#State = df_Repaired[['position']].copy()\n",
    "df_Repaired = df.rename({'Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]': 'RESP.ONLY_ONE_IN_GROUP[OHM]','Infinity|SPO2.SPO2_PULSE [COUNTS]': 'SPO2.SPO2_PULSE[COUNTS]'}, axis=1)\n",
    "FinalData = df_Repaired[['neonate','RESP.ONLY_ONE_IN_GROUP[OHM]', 'SPO2.SPO2_PULSE[COUNTS]','position']].dropna(axis = 0, thresh = 3).copy()\n",
    "State = FinalData[['position']].copy()\n",
    "Neonate = FinalData[['neonate']].copy()\n",
    "FinalData.drop('position', axis=1, inplace=True)\n",
    "FinalData.drop('neonate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Repaired = df.rename({'Infinity|ECG.ECG_LEAD_I [MICROVOLT]': 'ECG.ECG_LEAD_I[MICROVOLT]', 'Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]': 'RESP.ONLY_ONE_IN_GROUP[OHM]','Infinity|SPO2.SPO2_PULSE [COUNTS]': 'SPO2.SPO2_PULSE[COUNTS]'}, axis=1)#.dropna()\n",
    "#FinalData = df_Repaired[['ECG.ECG_LEAD_I[MICROVOLT]', 'RESP.ONLY_ONE_IN_GROUP[OHM]', 'SPO2.SPO2_PULSE[COUNTS]']].copy()\n",
    "#State = df_Repaired[['position']].copy()\n",
    "#df_Repaired = df.rename({'Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]': 'RESP.ONLY_ONE_IN_GROUP[OHM]','Infinity|SPO2.SPO2_PULSE [COUNTS]': 'SPO2.SPO2_PULSE[COUNTS]'}, axis=1)\n",
    "#FinalData = df_Repaired[['neonate','RESP.ONLY_ONE_IN_GROUP[OHM]', 'SPO2.SPO2_PULSE[COUNTS]','position']].dropna(axis = 0, thresh = 3).copy()\n",
    "#State = FinalData[['position']].copy()\n",
    "#Neonate = FinalData[['neonate']].copy()\n",
    "#FinalData.drop('position', axis=1, inplace=True)\n",
    "#FinalData.drop('neonate', axis=1, inplace=True)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].rename({'Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]': 'RESP.ONLY_ONE_IN_GROUP[OHM]','Infinity|SPO2.SPO2_PULSE [COUNTS]': 'SPO2.SPO2_PULSE[COUNTS]'}, axis=1)\n",
    "    print(data[i].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinalData.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].rename({'Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]': 'RESP.ONLY_ONE_IN_GROUP[OHM]','Infinity|SPO2.SPO2_PULSE [COUNTS]': 'SPO2.SPO2_PULSE[COUNTS]'}, axis=1)\n",
    "    print(f'Number of blank spaces for the {i} position: \\n {data[i].isna().sum()}')\n",
    "    print(data[i].dtypes.value_counts())\n",
    "    print(data[i].describe(), end = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinalData.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FinalData.groupby('neonate')['RESP.ONLY_ONE_IN_GROUP[OHM]'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FinalData.groupby('neonate')['SPO2.SPO2_PULSE[COUNTS]'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neonate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#'C:/Users/janja/Desktop/DaneMGR/21/B', 'C:/Users/janja/Desktop/DaneMGR/21/R'\n",
    "#fields = ['Infinity|RESP.ONLY_ONE_IN_GROUP [OHM]', 'Infinity|SPO2.SPO2_PULSE [COUNTS]']\n",
    "#df_local = pd.read_csv('C:/Users/janja/Desktop/DaneMGR/21/B/CsvLogBase_2022-06-08_124008.050_fast_Unknown.csv', sep = ',', encoding = 'UTF-8', usecols=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sweetviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the chosen columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "for i in range(len(data)):\n",
    "    orig_data_report = sv.analyze(data[i], pairwise_analysis = 'on')\n",
    "    orig_data_report.show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import math\n",
    "import numpy as np\n",
    "np_data = FinalData.to_numpy()\n",
    "labels = State.to_numpy()\n",
    "labels.astype('int')\n",
    "nb_timestamps, nb_sensors = np_data.shape\n",
    "window_size = 100 # Size of the data segments, earlier there was the value of 60\n",
    "timestamp_idx = 0 # Index along the timestamp dimension\n",
    "segment_idx = 0 # Index for the segment dimension\n",
    "\n",
    "\n",
    "nb_segments = int(math.floor(nb_timestamps/window_size))\n",
    "print('Starting segmentation with a window size of %d resulting in %d segments ...' % (window_size,nb_segments))\n",
    "data_to_save = np.zeros((nb_segments,window_size,nb_sensors),dtype=np.float32)\n",
    "labels_to_save = np.zeros(nb_segments,dtype=int)\n",
    "\n",
    "while segment_idx < nb_segments:\n",
    "    data_to_save[segment_idx] = np_data[timestamp_idx:timestamp_idx+window_size,:]\n",
    "    # Check the majority label ocurring in the considered window\n",
    "    current_labels = labels[timestamp_idx:timestamp_idx+window_size]\n",
    "    values, counts = np.unique(current_labels, return_counts=True)\n",
    "    labels_to_save[segment_idx] = values[np.argmax(counts)]\n",
    "    timestamp_idx += window_size\n",
    "    segment_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def segmentation(pd_data, shape):\n",
    "    np_data = pd_data.to_numpy()\n",
    "    nb_timestamps, nb_sensors = shape\n",
    "    window_size = 100 # Size of the data segments, earlier there was the value of 60\n",
    "    timestamp_idx = 0 # Index along the timestamp dimension\n",
    "    segment_idx = 0 # Index for the segment dimension\n",
    "    \n",
    "    nb_segments = int(math.floor(nb_timestamps/window_size))\n",
    "    print(f'Starting segmentation with a window size of {window_size} resulting in {nb_segments} segments.')\n",
    "    data_to_save = np.zeros((nb_segments,window_size,nb_sensors),dtype=np.float32)\n",
    "\n",
    "    while segment_idx < nb_segments:\n",
    "        data_to_save[segment_idx] = np_data[timestamp_idx:timestamp_idx+window_size,:]\n",
    "        timestamp_idx += window_size\n",
    "        segment_idx += 1\n",
    "    return data_to_save"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To poniżej powinno być tablicą, a na razie nie jest - czyta na razie tylko wartość (chociaż nawet to nie), ale nigdzie ich nie zapisuje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = []\n",
    "for i in range(len(data)):\n",
    "    shape = np.array(data[i]).shape\n",
    "    print(shape)\n",
    "    segmented = segmentation(data[i], shape)\n",
    "    print(segmented)\n",
    "    position.append(segmented)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(position)):\n",
    "    print(position[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(position)):\n",
    "    position[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(position)):\n",
    "    chunk_size = position[i].shape[1]\n",
    "    print(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "'''\n",
    "F = 200 # Data frequency of 200HZ\n",
    "labels = []\n",
    "def extract_features(data_to_save, chunk_size):\n",
    "    num_chunks = data_to_save.shape[0]//chunk_size\n",
    "    num_features = 6\n",
    "    num_sensors = data_to_save.shape[2]\n",
    "    features = np.zeros((num_chunks, num_features*num_sensors))\n",
    "    for i in range(num_chunks):\n",
    "        chunk = data_to_save[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "        mean = np.mean(chunk, axis=(0, 1))\n",
    "        median = np.median(chunk, axis=(0, 1))\n",
    "        std = np.std(chunk, axis=(0, 1))\n",
    "        min_val = np.min(chunk, axis=(0, 1))\n",
    "        max_val = np.max(chunk, axis=(0, 1))\n",
    "        #arg_max = np.argmax(chunk, axis=(0, 1))\n",
    "        sum_val = np.sum(chunk, axis=(0, 1))/F\n",
    "        features[i,:] = np.hstack([mean, median, std, min_val, max_val, sum_val]).reshape(1, num_features*num_sensors)\n",
    "        labels[i] = labels_to_save[i]\n",
    "    #features = features.reshape(num_chunks, num_features, num_sensors)\n",
    "    return features\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving array for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save_copy = position.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numba import cuda\n",
    "import scipy\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global num_features\n",
    "num_features = 9\n",
    "\n",
    "def extract_features(extracted_data, chunk_size = position[0].shape[1]):\n",
    "    freq = 100 # Data frequency of 200HZ\n",
    "    num_chunks = extracted_data.shape[0]//chunk_size\n",
    "    #global num_features\n",
    "    #num_features = 8\n",
    "    num_sensors = extracted_data.shape[2]\n",
    "    features = np.zeros((num_chunks, num_features*num_sensors))\n",
    "    \n",
    "    \n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    \n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        chunk = extracted_data[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "        mean = np.mean(chunk, axis=(0, 1))\n",
    "        median = np.median(chunk, axis=(0, 1))\n",
    "        std = np.std(chunk, axis=(0, 1))\n",
    "        min_val = np.min(chunk, axis=(0, 1))\n",
    "        max_val = np.max(chunk, axis=(0, 1))\n",
    "        sum_val = np.sum(chunk, axis=(0, 1))/freq\n",
    "        fft_sum_list = []\n",
    "                    # Perform FFT on the chunk of data\n",
    "        #argmax_list = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        fft_argmax_list = []\n",
    "        fft_sum_list_1 = []\n",
    "        row_fft = [np.array([]) for _ in range(num_sensors)]\n",
    "        '''\n",
    "\n",
    "    \n",
    "        fft_vals_1 = np.fft.fft(chunk[i,])\n",
    "        print(fft_vals_1)\n",
    "        fft_sum_1 = np.sum(np.abs(fft_vals_1))/freq\n",
    "                #print(fft_sum)\n",
    "                #print('The value of fft_sum: ', fft_sum)\n",
    "    \n",
    "            #print(fft_vals_1[:,0])    \n",
    "            #print(fft_vals_1[1,:])\n",
    "    \n",
    "            #print(fft_vals_1[i], end = '\\n')\n",
    "            #print('\\n')\n",
    "        fft_sum_list_1.append(fft_sum_1)\n",
    "        argmax_1 = np.argmax(fft_vals_1[i])\n",
    "            #print(argmax_1)\n",
    "            #print(fft_vals_1[argmax_1, i], end = '\\n')\n",
    "                #print(fft_vals[argmax, j], end = '\\n')\n",
    "            #print('\\n')\n",
    "        \n",
    "        #row_fft[0] = np.append(row_fft[0], fft_vals_1[:,0])\n",
    "        #row_fft[1] = np.append(row_fft[1], fft_vals_1[:,1])\n",
    "        '''\n",
    "        ###################################################################\n",
    "        for j in range(num_sensors):\n",
    "            kurtosis = scipy.stats.kurtosis(chunk[j,])\n",
    "            skew = scipy.stats.skew(chunk[j,])\n",
    "            \n",
    "            \n",
    "            fft_vals = np.fft.fft(chunk[j,])\n",
    "            #print(fft_vals)\n",
    "            fft_sum = np.sum(np.abs(fft_vals))/freq\n",
    "            #print(fft_sum)\n",
    "            #print('The value of fft_sum: ', fft_sum)\n",
    "    \n",
    "        \n",
    "        #print(fft_vals_1[i], end = '\\n')\n",
    "        #print('\\n')\n",
    "            fft_sum_list.append(fft_sum)\n",
    "            #argmax_1 = np.argmax(fft_vals_1[j])\n",
    "            \n",
    "        #print(fft_vals[:,0])    \n",
    "        #print(fft_vals[1,:])\n",
    "        positive_count = np.count_nonzero(fft_vals > 0)\n",
    "        negative_count = np.count_nonzero(fft_vals < 0)\n",
    "        positive += positive_count\n",
    "        negative += negative_count\n",
    "        print(\"Number of positive values:\", positive_count)\n",
    "        print(\"Number of negative values:\", negative_count)\n",
    "        \n",
    "        '''\n",
    "        fft_vals = np.fft.fft(chunk[j,])\n",
    "        fft_sum = np.sum(np.abs(fft_vals))/freq\n",
    "        #print(fft_sum)\n",
    "        #print('The value of fft_sum: ', fft_sum)\n",
    "        print(fft_vals, end = '\\n')\n",
    "        print('\\n')\n",
    "        fft_sum_list.append(fft_sum)\n",
    "        argmax = np.argmax(fft_vals)\n",
    "        print(argmax)\n",
    "        print(fft_vals[argmax], end = '\\n')\n",
    "        #print(fft_vals[argmax, j], end = '\\n')\n",
    "        print('\\n')\n",
    "        #if (fft_vals[argmax] > 0):\n",
    "        #    fft_argmax_list.append(fft_vals[argmax])\n",
    "        '''     \n",
    "        #print(row_fft[0])\n",
    "            #print(row_fft[1])       \n",
    "            \n",
    "            #val_for_argmax = fft_vals[argmax]\n",
    "                        #fft_argmax_list.append(argmax_test)\n",
    "                        #print(argmax_test)\n",
    "                        #argmax = np.argmax(chunk[j,])\n",
    "                        #argmax_list.append(argmax)\n",
    "        #print(\"fft_argmax_list\", fft_argmax_list)\n",
    "        #print(\"Argmax list\", argmax_list)\n",
    "        fft_sum_list = np.array(fft_sum_list)\n",
    "        #####print(fft_sum_list)    \n",
    "        #print(fft_argmax_list)\n",
    "        features[i,:] = np.concatenate([mean, median, std, min_val, max_val, sum_val, kurtosis, skew, fft_sum_list])\n",
    "        global feature_names\n",
    "        feature_names = ['mean', 'mean_2', 'median', 'median_2', 'std', 'std_2', 'min_val', 'min_val_2',\\\n",
    "            'max_val', 'max_val_2', 'sum_val', 'sum_val_2', 'kurtosis', 'kurtosis_2', 'skew', 'skew_2', 'fft_sum', 'fft_sum_2']\n",
    "    print(positive)\n",
    "    print(negative)        \n",
    "    return features\n",
    "\n",
    "####\n",
    "\n",
    "#Uwaga obie wartości fft_sum dla obydwu czujników dopisywane są na sam koniec tablicy, jedna po drugiej!!\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "# Number of samples\n",
    "#N = new_array.shape[0]\n",
    "\n",
    "# Compute the frequencies in Hz for the power spectrum\n",
    "#frequencies = np.fft.fftfreq(N, 1/sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive values: 100\n",
      "Number of negative values: 100\n",
      "Number of positive values: 100\n",
      "Number of negative values: 100\n",
      "200\n",
      "200\n",
      "Number of positive values: 100\n",
      "Number of negative values: 100\n",
      "Number of positive values: 99\n",
      "Number of negative values: 101\n",
      "199\n",
      "201\n"
     ]
    }
   ],
   "source": [
    "#extract_features(position[0])\n",
    "\n",
    "\n",
    "for i in range(len(position)):\n",
    "        extract_features(position[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extract_features(data_to_save, chunk_size):\n",
    "    num_chunks = data_to_save.shape[0]//chunk_size\n",
    "    num_features = 4\n",
    "    num_sensors = data_to_save.shape[2]\n",
    "    features = np.zeros((num_chunks, num_features*num_sensors))\n",
    "    for i in range(num_chunks):\n",
    "        chunk = data_to_save[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "        mean = np.mean(chunk, axis=(0, 1))\n",
    "        median = np.median(chunk, axis=(0, 1))\n",
    "        std = np.std(chunk, axis=(0, 1))\n",
    "        min_val = np.min(chunk, axis=(0, 1))\n",
    "        #arg_max = np.argmax(chunk, axis=(0, 1))\n",
    "        features[i,:] = np.hstack([mean, median, std, min_val]).flatten()\n",
    "    features = features.reshape(num_chunks, num_features, num_sensors)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling():\n",
    "    extracted_features = []\n",
    "    for i in range(len(position)):\n",
    "        extracted_features.append(extract_features(position[i]))\n",
    "        #print(extracted_features)\n",
    "    \n",
    "    position_0 = extract_features(position[0])\n",
    "    position_1 = extract_features(position[1])\n",
    "    \n",
    "    labels_0 = np.zeros(position_0.shape[0])\n",
    "    labels_1 = np.ones(position_1.shape[0])\n",
    "    \n",
    "    complete_feature_dataset = np.concatenate((position_0,position_1))\n",
    "    complete_label_dataset = np.concatenate((labels_0, labels_1))\n",
    "    \n",
    "    return complete_feature_dataset, complete_label_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position_0 = extract_features(position[0])\n",
    "#position_1 = extract_features(position[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position_0.size\n",
    "#position_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_0 = np.zeros(position_0.shape[0])\n",
    "#labels_1 = np.ones(position_1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_feature_dataset = np.concatenate((position_0,position_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_label_dataset = np.concatenate((labels_0, labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_feature_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_label_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_label_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#labels = np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_array = new_array.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(labels_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating an additional table called Saved_data for further actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved_data = complete_feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_to_save = np.array(data_to_save) # this is the 3D array\n",
    "#data_to_save = data_to_save.reshape(data_to_save.shape[0], -1) # reshape to 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_to_save = labels\n",
    "#labels_to_save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def data_shuffling():\n",
    "    dataset = labeling()\n",
    "    shuffler = np.random.permutation(len(dataset[0]))\n",
    "    X = dataset[0][shuffler]\n",
    "    y = dataset[1][shuffler]\n",
    "\n",
    "    return X,y\n",
    "#le = LabelEncoder()\n",
    "#y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_shuffling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(model, 6)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = RandomForestClassifier()\n",
    "rfe = RFE(model, 6)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# create the RFE model and select 6 attributes\n",
    "svm = SVC(kernel=\"linear\", C=1)\n",
    "rfe = RFE(svm, 6)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#models = [LogisticRegression(solver='liblinear', max_iter=1000), SVC(kernel='linear'), RandomForestClassifier()]\n",
    "models = [LogisticRegression(solver='liblinear', max_iter=1000), RandomForestClassifier()]\n",
    "\n",
    "for model in models:\n",
    "    rfe = RFE(model, n_features_to_select=6)\n",
    "    cv_scores = cross_val_score(rfe, X, y, cv=5)\n",
    "    print(\"Model:\", model.__class__.__name__)\n",
    "    print(\"Selected Features:\", rfe.fit(X, y).support_)\n",
    "    print(\"CV Scores:\", cv_scores)\n",
    "    print(\"Mean CV Score:\", cv_scores.mean())\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_elimination(n = i):\n",
    "    # Create the Random Forest classifier\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    # Perform feature selection using RFE\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n, step=1)\n",
    "    shuffled_dataset = data_shuffling()\n",
    "    X = shuffled_dataset[0]\n",
    "    y = shuffled_dataset[1]\n",
    "    rfe.fit(X, y)\n",
    "\n",
    "    # Get the selected feature indices\n",
    "    selected_features = rfe.support_\n",
    "    selected_features_indices = np.where(selected_features)[0]\n",
    "    print(selected_features_indices)\n",
    "    print('Number of features selected: %d' % (n))\n",
    "    \n",
    "    names_selected_features = []\n",
    "    '''\n",
    "    for i in range(len(selected_features_indices)):\n",
    "        if (selected_features_indices == feature_names[i]):\n",
    "            names_selected_features.concatenate()\n",
    "    print(feature_names)\n",
    "    '''\n",
    "    for i in selected_features_indices:\n",
    "        names_selected_features.append(feature_names[i])\n",
    "    #print(feature_names)\n",
    "    print(names_selected_features)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    # Use the selected features to train and evaluate the classifier\n",
    "    global X_selected\n",
    "    X_selected = X[:, selected_features]\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Train the Random Forest classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy: %.4f%%\" % (accuracy * 100.0))\n",
    "    print(\"Precision: %.4f%%\" % (precision * 100.0))\n",
    "    print(\"Recall: %.4f%%\" % (recall * 100.0))\n",
    "    print(\"Mean Absolute Error:\", mae, end = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def xyz():\n",
    "for i in range(num_features*2,0,-1):\n",
    "    feature_elimination(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code opens a mini window in the upper part of the screen\n",
    "n = int(input(\"Choose, with how many features do you want to continue\"))\n",
    "feature_elimination(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(complete_label_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set(y)\n",
    "#y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.size)\n",
    "print(X_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.size)\n",
    "print(y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "rfc_prediction = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, rfc_prediction)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, rfc_prediction)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, rfc_prediction)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, rfc_prediction)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# create the model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# create the model\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create an instance of the MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=42,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Create an instance of the KFold class\n",
    "kf = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RFC model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_RFC = []\n",
    "precision_scores_RFC = []\n",
    "recall_scores_RFC = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rfc.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_RFC.append(acc)\n",
    "    precision_scores_RFC.append(precision)\n",
    "    recall_scores_RFC.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_RFC))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_RFC))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_RFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGboost model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_XGB = []\n",
    "precision_scores_XGB = []\n",
    "recall_scores_XGB = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_XGB.append(acc)\n",
    "    precision_scores_XGB.append(precision)\n",
    "    recall_scores_XGB.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_XGB))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_XGB))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_XGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVM model\n",
    "SVM = svm.SVC(kernel='rbf', C=1, gamma='scale')\n",
    "\n",
    "# Initialize a list to store the accuracy scores\n",
    "acc_scores_SVC = []\n",
    "precision_scores_SVC = []\n",
    "recall_scores_SVC = []\n",
    "\n",
    "# Perform the K-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    SVM.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = SVM.predict(X_test)\n",
    "\n",
    "    # Calculate the scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc_scores_SVC.append(acc)\n",
    "    precision_scores_SVC.append(precision)\n",
    "    recall_scores_SVC.append(recall)\n",
    "\n",
    "# Print the mean scores\n",
    "print(\"Mean accuracy:\", np.mean(acc_scores_SVC))\n",
    "print(\"Mean precision:\", np.mean(precision_scores_SVC))\n",
    "print(\"Mean recall:\", np.mean(recall_scores_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Compute the FFT of the signal along the first axis\n",
    "fft = np.fft.fft(Saved_data, axis=0)\n",
    "\n",
    "# Compute the power spectrum\n",
    "power_spectrum = np.abs(fft)**2\n",
    "\n",
    "# Plot the power spectrum of 5 slices along the first axis\n",
    "for i in range(25):\n",
    "    plt.plot(power_spectrum[:, i, 0])\n",
    "    plt.xlabel('Frequency (bins)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal (Slice {i})')\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the FFT of the signal along the first axis for channel 0\n",
    "fft0 = np.fft.fft(Saved_data[:,:,0], axis=0)\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for channel 1\n",
    "fft1 = np.fft.fft(Saved_data[:,:,1], axis=0)\n",
    "\n",
    "# Compute the power spectrum for channel 0\n",
    "power_spectrum0 = np.abs(fft0)**2\n",
    "\n",
    "# Compute the power spectrum for channel 1\n",
    "power_spectrum1 = np.abs(fft1)**2\n",
    "\n",
    "plots = 25\n",
    "# Plot the power spectrum of 25 slices along the first axis for channel 0\n",
    "for i in range(plots):\n",
    "    plt.plot(power_spectrum0[:, i])\n",
    "    plt.xlabel('Frequency (bins)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 0 (Slice {i})')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the power spectrum of 5 slices along the first axis for channel 1\n",
    "for i in range(plots):\n",
    "    plt.plot(power_spectrum1[:, i])\n",
    "    plt.xlabel('Frequency (bins)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 1 (Slice {i})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the FFT of the signal along the first axis for channel 0\n",
    "fft0 = np.fft.fft(Saved_data[:,:,0], axis=0)\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for channel 1\n",
    "fft1 = np.fft.fft(Saved_data[:,:,1], axis=0)\n",
    "\n",
    "# Compute the power spectrum for channel 0\n",
    "power_spectrum0 = np.abs(fft0)**2\n",
    "\n",
    "# Compute the power spectrum for channel 1\n",
    "power_spectrum1 = np.abs(fft1)**2\n",
    "\n",
    "# Sampling rate\n",
    "sampling_rate = 200\n",
    "\n",
    "# Number of samples\n",
    "N = Saved_data.shape[0]\n",
    "\n",
    "# Compute the frequencies in Hz for the power spectrum\n",
    "frequencies0 = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "frequencies1 = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "\n",
    "plots = 25\n",
    "# Plot the power spectrum of 25 slices along the first axis for channel 0\n",
    "for i in range(plots):\n",
    "    plt.plot(frequencies0, power_spectrum0[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 0 (Slice {i})')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the power spectrum of 5 slices along the first axis for channel 1\n",
    "for i in range(plots):\n",
    "    plt.plot(frequencies1, power_spectrum1[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Signal Channel 1 (Slice {i})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots prepared for newly preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the FFT of the signal along the first axis for each sensor\n",
    "fft = np.fft.fft(new_array, axis=0)\n",
    "\n",
    "# Compute the power spectrum for each sensor\n",
    "power_spectrum = np.abs(fft)**2\n",
    "\n",
    "# Sampling rate\n",
    "sampling_rate = 1\n",
    "\n",
    "# Number of samples\n",
    "N = new_array.shape[0]\n",
    "\n",
    "# Compute the frequencies in Hz for the power spectrum\n",
    "frequencies = np.fft.fftfreq(N, 1/sampling_rate)\n",
    "\n",
    "# Plot the power spectrum for each sensor\n",
    "for i in range(new_array.shape[1]):\n",
    "    plt.plot(frequencies, power_spectrum[:, i])\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.xlim(left=0)\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(f'Power Spectrum of Sensor {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
