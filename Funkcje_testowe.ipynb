{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1258702, 100, 2)\n",
    "(1292542, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "array1 = np.random.rand(6, 4, 2)\n",
    "array2 = np.random.rand(6, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [array1, array2]\n",
    "print(len(dataset))\n",
    "print(len(dataset[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global num_features\n",
    "num_features = 2\n",
    "\n",
    "def extract_features(extracted_data, chunk_size = dataset[0].shape[1]):\n",
    "    freq = 100 # Data frequency of 200HZ\n",
    "    num_chunks = extracted_data.shape[0]//chunk_size\n",
    "    num_sensors = extracted_data.shape[2]\n",
    "    features = np.zeros((num_chunks, num_features*num_sensors))\n",
    "    \n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    \n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        chunk = extracted_data[i*chunk_size:(i+1)*chunk_size, :, :]\n",
    "\n",
    "        fft_sum_list = []\n",
    "        fft_sum_list_1 = []\n",
    "\n",
    "        fft_vals = np.fft.fft(chunk[i,])\n",
    "        print(f'fft_vals {fft_vals}')\n",
    "\n",
    "\n",
    "\n",
    "        for j in range(num_sensors):                \n",
    "            fft_vals_1 = np.fft.fft(chunk[j,])\n",
    "            #print(fft_vals_1)\n",
    "            fft_sum_1 = np.sum(np.abs(fft_vals_1))/freq\n",
    "\n",
    "            fft_sum_list_1.append(fft_sum_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        positive_count = np.count_nonzero(fft_vals_1 > 0)\n",
    "        negative_count = np.count_nonzero(fft_vals_1 < 0)\n",
    "        positive += positive_count\n",
    "        negative += negative_count\n",
    "        print(\"Number of positive values:\", positive_count)\n",
    "        print(\"Number of negative values:\", negative_count)\n",
    "\n",
    "\n",
    "        fft_sum_list = np.array(fft_sum_list)\n",
    "\n",
    "        features[i,:] = np.concatenate([fft_sum_list])\n",
    "        global feature_names\n",
    "        feature_names = ['mean', 'mean_2', 'median', 'median_2', 'std', 'std_2', 'min_val', 'min_val_2',\\\n",
    "            'max_val', 'max_val_2', 'sum_val', 'sum_val_2', 'kurtosis', 'kurtosis_2', 'skew', 'skew_2', 'fft_sum', 'fft_sum_2']\n",
    "    print(f'total positive values for position {positive}')\n",
    "    print(f'total negative values for position {negative}')        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100\n",
    "array4 = np.random.rand(200, 100, 2)\n",
    "num_chunks = array4.shape[0]//chunk_size\n",
    "print(num_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(dataset)):\n",
    "    print(len(dataset[i]))\n",
    "    for j in range(len(dataset[i])):\n",
    "        #mean = np.mean(dataset[i][j], axis=(0, 1))\n",
    "        for k in range(dataset[i].shape[2]):\n",
    "        #    print(dataset[j][k])\n",
    "        #    print('\\n')\n",
    "        \n",
    "        \n",
    "            mean = np.mean(dataset[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.49082608 0.32714676]\n",
      "  [0.40190756 0.17331429]]\n",
      "\n",
      " [[0.05320489 0.16290769]\n",
      "  [0.15483338 0.29263004]]]\n",
      "mean\n",
      "[0.44636682 0.25023053]\n"
     ]
    }
   ],
   "source": [
    "array5 = np.random.rand(2, 2, 2)\n",
    "print(array5)\n",
    "mean_array = np.mean(array5[0], axis=0)\n",
    "print(\"mean\")\n",
    "print(mean_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4463668192720745\n",
      "0.2502305255798803\n"
     ]
    }
   ],
   "source": [
    "print(mean_array[0])\n",
    "print(mean_array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "global num_features\n",
    "num_features = 6\n",
    "def feature_extraction(data):\n",
    "    freq = 100\n",
    "    mean_array = np.empty((len(data),2), dtype = float)\n",
    "    median_array = np.empty((len(data),2), dtype = float)\n",
    "    std_array = np.empty((len(data),2), dtype = float)\n",
    "    min_val_array = np.empty((len(data),2), dtype = float)\n",
    "    max_val_array = np.empty((len(data),2), dtype = float)\n",
    "    sum_val_array = np.empty((len(data),2), dtype = float)\n",
    "    for i in range(len(data)):\n",
    "        mean_array[i] = np.mean(data[i], axis=0)\n",
    "        median_array[i] = np.median(data[i], axis=0)\n",
    "        std_array[i] = np.std(data[i], axis=0)\n",
    "        min_val_array[i] = np.min(data[i], axis=0)\n",
    "        max_val_array[i] = np.max(data[i], axis=0)\n",
    "        sum_val_array[i] = np.sum(data[i], axis=0)/freq\n",
    "        \n",
    "    \n",
    "    mean_sensor_1 = mean_array[:,0]\n",
    "    mean_sensor_2 = mean_array[:,1]\n",
    "    median_sensor_1 = median_array[:,0]\n",
    "    median_sensor_2 = median_array[:,1]\n",
    "    std_sensor_1 = std_array[:,0]\n",
    "    std_sensor_2 = std_array[:,1]\n",
    "    min_sensor_1 = min_val_array[:,0]\n",
    "    min_sensor_2 = min_val_array[:,1]\n",
    "    max_sensor_1 = max_val_array[:,0]\n",
    "    max_sensor_2 = max_val_array[:,1]\n",
    "    sum_sensor_1 = sum_val_array[:,0]\n",
    "    sum_sensor_2 = sum_val_array[:,1]\n",
    "    print(mean_sensor_1)\n",
    "    print(sum_sensor_2)\n",
    "    \n",
    "    features = np.zeros((len(data), num_features*2))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        features[i,:] = np.concatenate([mean_sensor_1, mean_sensor_2, median_sensor_1, median_sensor_2, std_sensor_1, std_sensor_2,\\\n",
    "            min_sensor_1, min_sensor_2, max_sensor_1, max_sensor_2, sum_sensor_1, sum_sensor_2])\n",
    "        \n",
    "    global feature_names\n",
    "    feature_names = ['mean', 'mean_2', 'median', 'median_2', 'std', 'std_2', 'min_val', 'min_val_2',\\\n",
    "        'max_val', 'max_val_2', 'sum_val', 'sum_val_2', 'kurtosis', 'kurtosis_2', 'skew', 'skew_2', 'fft_sum', 'fft_sum_2']\n",
    "    \n",
    "    \n",
    "        \n",
    "    #print(mean_sensor_2)\n",
    "    #print(mean_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44636682 0.25023053]\n",
      " [0.10401913 0.22776887]]\n",
      "0.4463668192720745\n",
      "0.2502305255798803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.44636682, 0.25023053])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extraction(array5)\n",
    "np.mean(array5[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44636682 0.10401913]\n",
      "[0.00500461 0.00455538]\n"
     ]
    }
   ],
   "source": [
    "feature_extraction(array5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_vals = np.fft.fft(chunk[i,])\n",
    "        print(f'fft_vals {fft_vals}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
